# Image captioning in Cultural Heritage

[![PyTorch Lightning](https://img.shields.io/badge/PyTorch-Lightning-blueviolet)](#)

Project work for Computer Vision exam. Evaluation of image captioning and visual question answering techniques on cultural heritage datasets

<!-- TOC -->
* [Image captioning in Cultural Heritage](#image-captioning-in-cultural-heritage)
  * [Project Structure](#project-structure)
  * [Data](#data)
  * [Usage](#usage)
    * [Configuration](#configuration)
    * [Train](#train)
    * [Validation](#validation)
  * [Results](#results)
    * [Artpedia](#artpedia-)
      * [zero-shot](#zero-shot)
      * [finetuning](#finetuning)
  * [TODOs](#todos)
  * [References](#references)
<!-- TOC -->

## Project Structure
Here is the description of the main files and folders of the project.

```
  cultural-heritage-image2text/
  │
  ├── main.py - main script for training and testing models
  │
  ├── notebooks/ - collection of notebooks for exploration and demonstration of features
  │   ├── image-captioning.ipynb: Artpedia dataset exploration and example of image captioning with trained models
  │   └── ...
  │
  ├── data_loader/ - anything about data loading goes here
  │   └── artpedia.py contains Artpedia Dataset and DataModule
  │
  ├── data/ - default directory for storing input data
  │
  ├── model/ - models and metrics
  │   ├── model.py - LightningModule wrapper for image captioning
  │   └── metrics/ directory with custom metrics
  │
  ├── runs/
  │   ├── cultural-heritage/ - trained models are saved here
  │   └── wandb/ - local logdir for wandb and logging output
  │
  └── utils/
      ├── utils.py - small utility functions for training
      └── download.py - utility to download images from Artpedia json metadata
 ```

## Data
Experiments were performed on the Artpedia [[1](https://iris.unimore.it/retrieve/handle/11380/1178736/224456/paper.pdf)] dataset. Images were downloaded from Wikipedia using the [download.py](download.py) script.
To download the images, run the following command, providing a valid identifier to avoid being blocked by the server.

```bash
python utils/download.py email@domain.com --ann_file data/artpedia/artpedia.json --img_dir data/artpedia/images 
```

The already processed annotations (including both original Artpedia and the augmented dataset) are provided [here](https://drive.google.com/drive/folders/1STLtxx81r4VUCIqU3_3olhqGBSxf2sXh?usp=share_link).

## Usage
Command line interface is implemented using [LightningCLI](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.cli.LightningCLI.html).

### Configuration
The setup during training and validation is controlled by a configuration file. 
The configuration file is a YAML file with the following structure:

```yaml
# lightning.pytorch==2.0.1.post0
seed_everything: int | bool
trainer:
  # list of trainer args
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      # wandb logging args
  callbacks:
    class_path: callbacks.predictions.LogPredictionSamplesCallback
model:
  model_name_or_path: microsoft/git-base
  learning_rate: 5.0e-05
  warmup_steps: 500
  weight_decay: 0.0
  metrics:
    # add or remove metrics here
    - class_path: model.CocoScore
    - class_path: torchmetrics.text.BERTScore
      init_args:
        model_name_or_path: distilbert-base-uncased
        batch_size: 16
        lang: en
        max_length: 512
  generation:
    # generation args
data:
  img_dir: data/artpedia/
  ann_file: data/artpedia/artpedia_augmented.json
  batch_size: 2
  # Processor name for model
  model_name_or_path: microsoft/git-base
  num_workers: 6
ckpt_path: null # provide a path to a checkpoint to load
```

Every configuration can be overridden by passing a command line argument with the same name. For example, to override the `batch_size` parameter, you can run:

```bash
python main.py fit --config configs/config.yaml --data.batch_size 32
```

You can find a complete example of a configuration file in [configs/config.yaml](configs/config.yaml)

### Train
Training is performed using the `fit` command, followed by the path to the configuration file and other optional arguments.

```bash
python main.py fit --config configs/config.yaml
```

### Validation
Validation is performed using the `validate` command, followed by the path to the configuration file and other optional arguments.
```bash
python main.py validate --config configs/config.yaml --ckpt_path path/to/ckpt.ckpt
```

## Results

### Artpedia 
#### zero-shot
Here are the performance of the pretrained models on the Artpedia dataset. The caption is generated by the concatenating all the `visual_sentences`.

| Model                                                                        | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE | BERTScore (f1) |
|------------------------------------------------------------------------------|--------|--------|--------|--------|--------|---------|-------|-------|----------------|
| [GIT-base](https://huggingface.co/docs/transformers/model_doc/git)           | 0      | 0      | 0      | 0      | 0.01   | 0.07    | 0.004 | -     | 0.53           |
| [GIT-large-Textcaps](https://huggingface.co/docs/transformers/model_doc/git) | 0.004  | 0.001  | 0.001  | 0      | 0.03   | 0.12    | 0.026 | -     | 0.60           |

[//]: # (| [OFA]&#40;https://github.com/OFA-Sys/OFA&#41;                                        |        |        |        |        |        |         |       |       |           |)
[//]: # (| [BLIP]&#40;https://huggingface.co/docs/transformers/model_doc/blip&#41;              |        |        |        |        |        |         |       |       |           |)

#### finetuning
Models have been finetuned for 250 steps on training dataset.

| Model                                                              | Augmented | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE | BERTScore (f1) |
|--------------------------------------------------------------------|-----------|--------|--------|--------|--------|--------|---------|-------|-------|----------------|
| OFA (Bongini et al [[2](https://arxiv.org/pdf/2207.12101.pdf)])                                        | ✗         | 0.048  | -      | -      | -      | -      | 0.138   | 0.091 | -     | -              |
| GPT-3 (Bongini et al [[2](https://arxiv.org/pdf/2207.12101.pdf)])                                      | ✗         | 0.181  | -      | -      | -      | -      | 0.188   | 0.079 | -     | -              |
| [GIT-base](https://huggingface.co/docs/transformers/model_doc/git) | ✗         | 0.04   | 0.02   | 0.01   | 0.004  | 0.03   | 0.16    | 0.03  | -     | 0.65           |
| [GIT-base](https://huggingface.co/docs/transformers/model_doc/git) | ✓         | 0.06   | 0.03   | 0.008  | 0.005  | 0.054  | 0.17    | 0.04  | -     | 0.65           |

[//]: # (| [OFA]&#40;https://github.com/OFA-Sys/OFA&#41;                           |        |        |        |        |        |         |       |       |           |)
[//]: # (| [BLIP]&#40;https://huggingface.co/docs/transformers/model_doc/blip&#41; |        |        |        |        |        |         |       |       |           |)

## TODOs
- [x] Train on Artpedia
- [x] Add BLEU, METEOR, ROUGE-L, CIDEr
- [ ] Add SPICE
- [x] Handle validation loss calculation during generation
- [ ] Support OFA (Tiny-Medium-Base)
- [x] Support BLIP
- [x] Evaluate on other art datasets (ArtCap, SemArt, IconClass, ...)

## References
- [[1](https://iris.unimore.it/retrieve/handle/11380/1178736/224456/paper.pdf)] Stefanini, Matteo, et al. "Artpedia: A new visual-semantic dataset with visual and contextual sentences in the artistic domain." Image Analysis and Processing–ICIAP 2019: 20th International Conference, Trento, Italy, September 9–13, 2019, Proceedings, Part II 20. Springer International Publishing, 2019. -  dataset [link](https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=35)
- [[2](https://arxiv.org/pdf/2207.12101.pdf)] Bongini, Pietro, Federico Becattini, and Alberto Del Bimbo. "Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?." arXiv preprint arXiv:2207.12101 (2022).
- [[3](https://iopscience.iop.org/article/10.1088/1757-899X/949/1/012074/meta)] Bongini, Pietro, et al. "Visual question answering for cultural heritage." IOP Conference Series: Materials Science and Engineering. Vol. 949. No. 1. IOP Publishing, 2020.
- [[4](https://arxiv.org/abs/2205.14100)] Wang, Jianfeng et al. “GIT: A Generative Image-to-text Transformer for Vision and Language.” Trans. Mach. Learn. Res. 2022 (2022): n. pag. - [Huggingface page](https://huggingface.co/docs/transformers/model_doc/git)
- [[5](https://proceedings.mlr.press/v162/wang22al/wang22al.pdf)] Wang, Peng, et al. "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework." International Conference on Machine Learning. PMLR, 2022.
- [[6](https://arxiv.org/pdf/2302.07944.pdf)] Trabucco, Brandon, et al. "Effective data augmentation with diffusion models." arXiv preprint arXiv:2302.07944 (2023).
- [[7](https://arxiv.org/pdf/2210.07574.pdf)] He, Ruifei, et al. "Is synthetic data from generative models ready for image recognition?." arXiv preprint arXiv:2210.07574 (2022). 