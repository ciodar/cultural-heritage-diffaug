# Image captioning in Cultural Heritage

[![PyTorch Lightning](https://img.shields.io/badge/PyTorch-Lightning-blueviolet)](#)

Project work for Computer Vision exam. Evaluation of image captioning and visual question answering techniques on cultural heritage datasets

## Project Structure
Here is the description of the main files and folders of the project.

```
  cultural-heritage-image2text/
  │
  ├── main.py - main script for training and testing models
  │
  ├── notebooks/ - collection of notebooks for exploration and demonstration of features
  │   ├── image-captioning.ipynb: Artpedia dataset exploration and example of image captioning with trained models
  │   └── ...
  │
  ├── data_loader/ - anything about data loading goes here
  │   └── artpedia.py contains Artpedia Dataset and DataModule
  │
  ├── data/ - default directory for storing input data
  │
  ├── model/ - models and metrics
  │   ├── model.py - LightningModule wrapper for image captioning
  │   └── metrics/ directory with custom metrics
  │
  ├── runs/
  │   ├── cultural-heritage/ - trained models are saved here
  │   └── wandb/ - local logdir for wandb and logging output
  │
  └── utils/
      ├── utils.py - small utility functions for training
      └── download.py - utility to download images from Artpedia json metadata
 ```

## Data
Experiments were performed on the [Artpedia](https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=35) dataset. Images were downloaded from Wikipedia using the [download.py](utils/download.py) script.
To download the images, run the following command, providing a valid identifier to avoid being blocked by the server.

```bash
python utils/download.py email@domain.com --ann_file data/artpedia/artpedia.json --img_dir data/artpedia/images 
```

The already processed annotations (including both original artpedia and the augmented dataset) are provided [here](https://drive.google.com/drive/folders/1STLtxx81r4VUCIqU3_3olhqGBSxf2sXh?usp=share_link).

## Usage
Command line interface is implemented using [LightningCLI](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.cli.LightningCLI.html).

### Configuration
The setup during training and validation is controlled by a configuration file. 
The configuration file is a YAML file with the following structure:

```yaml
# lightning.pytorch==2.0.1.post0
seed_everything: int | bool
trainer:
  # list of trainer args
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      # wandb logging args
  callbacks:
    class_path: callbacks.predictions.LogPredictionSamplesCallback
model:
  model_name_or_path: microsoft/git-base
  learning_rate: 5.0e-05
  warmup_steps: 500
  weight_decay: 0.0
  metrics:
    # add or remove metrics here
    - class_path: model.CocoScore
    - class_path: torchmetrics.text.BERTScore
      init_args:
        model_name_or_path: distilbert-base-uncased
        batch_size: 16
        lang: en
        max_length: 512
  generation:
    # generation args
data:
  img_dir: data/artpedia/
  ann_file: data/artpedia/artpedia_augmented.json
  batch_size: 2
  # Processor name for model
  model_name_or_path: microsoft/git-base
  num_workers: 6
ckpt_path: null # provide a path to a checkpoint to load
```

Every configuration can be overridden by passing a command line argument with the same name. For example, to override the `batch_size` parameter, you can run:

```bash
python main.py fit --config configs/config.yaml --data.batch_size 32
```

You can find a complete example of a configuration file in [configs/config.yaml](configs/config.yaml)

### Train
Training is performed using the `fit` command, followed by the path to the configuration file and other optional arguments.

```bash
python main.py fit --config configs/config.yaml
```

### Validation
Validation is performed using the `validate` command, followed by the path to the configuration file and other optional arguments.
```bash
python main.py validate --config configs/config.yaml --ckpt_path path/to/ckpt.ckpt
```

## Results

### Artpedia 
#### zero-shot
Here are the performance of the pretrained models on the Artpedia dataset. The caption is generated by the concatenating all the `visual_sentences`.

| Model                                                                        | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE | BERTScore |
|------------------------------------------------------------------------------|--------|--------|--------|--------|--------|---------|-------|-------|-----------|
| [GIT-base](https://huggingface.co/docs/transformers/model_doc/git)           | 0      | 0      | 0      | 0      | 0.01   | 0.07    | 0.004 | -     | 0.53      |
| [GIT-large-Textcaps](https://huggingface.co/docs/transformers/model_doc/git) | 0.04   | 0.001  | 0.001  | 0      | 0.03   | 0.12    | 0.02  | -     | 0.60      |

[//]: # (| [OFA]&#40;https://github.com/OFA-Sys/OFA&#41;                                        |        |        |        |        |        |         |       |       |           |)
[//]: # (| [BLIP]&#40;https://huggingface.co/docs/transformers/model_doc/blip&#41;              |        |        |        |        |        |         |       |       |           |)

#### finetuning
Models have been finetuned for 10 epochs on training dataset.

| Model                                                              | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE | BERTScore |
|--------------------------------------------------------------------|--------|--------|--------|--------|--------|---------|-------|-------|-----------|
| [GIT-base](https://huggingface.co/docs/transformers/model_doc/git) | 0.05   | 0.02   | 0.005  | 0.002  | 0.05   | 0.14    | 0.01  | -     | 0.62      |

[//]: # (| [OFA]&#40;https://github.com/OFA-Sys/OFA&#41;                           |        |        |        |        |        |         |       |       |           |)
[//]: # (| [BLIP]&#40;https://huggingface.co/docs/transformers/model_doc/blip&#41; |        |        |        |        |        |         |       |       |           |)

#### finetuning with augmentation
Here are the results of the finetuning with augmentation.

| Model                                                              | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR | ROUGE-L | CIDEr | SPICE | BERTScore |
|--------------------------------------------------------------------|--------|--------|--------|--------|--------|---------|-------|-------|-----------|
| [GIT-base](https://huggingface.co/docs/transformers/model_doc/git) | 0.02   | 0.01   | 0.004  | 0.002  | 0.04   | 0.15    | 0.04  | -     | 0.65      |

[//]: # (| [OFA]&#40;https://github.com/OFA-Sys/OFA&#41;                           |        |        |        |        |        |         |       |       |           |)
[//]: # (| [BLIP]&#40;https://huggingface.co/docs/transformers/model_doc/blip&#41; |        |        |        |        |        |         |       |       |           |)

## TODOs
- [x] Train on Artpedia
- [x] Add BLEU, METEOR, ROUGE-L, CIDEr
- [ ] Add SPICE
- [ ] Handle validation loss calculation during generation
- [ ] Support OFA (Tiny-Medium-Base)
- [ ] Check if it is possible to shorten caption with some preprocessing
- [ ] Evaluate on other art datasets

## References
- [Artpedia](https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=35) dataset - [paper](https://iris.unimore.it/retrieve/handle/11380/1178736/224456/paper.pdf)
- [GIT](https://huggingface.co/docs/transformers/model_doc/git) model - [paper](https://arxiv.org/abs/2205.14100)

